import feedparser
import requests
import json
import os
import re
from bs4 import BeautifulSoup
import time
import hashlib

# --- è¨­å®šã‚¨ãƒªã‚¢ ---
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
DATA_FILE = "data.json"

# ãƒ–ãƒ©ã‚¦ã‚¶ã®ãµã‚Šã‚’ã™ã‚‹ãƒ˜ãƒƒãƒ€ãƒ¼
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# å·¡å›ãƒªã‚¹ãƒˆï¼ˆå‰å›ã¨åŒã˜æœ€å¼·ãƒªã‚¹ãƒˆï¼‰
RSS_URLS = [
    "https://prtimes.jp/gourmet.rdf",
    "https://prtimes.jp/technology.rdf",
    "https://prtimes.jp/app.rdf",
    "https://prtimes.jp/entertainment.rdf",
    "https://touchlab.jp/feed/",
    "https://www.gizmodo.jp/index.xml",
    "https://corriente.top/feed/",
    "https://www.lifehacker.jp/feed/index.xml",
    "https://rocketnews24.com/feed/",
    "https://automaton-media.com/feed/",
]

TARGET_KEYWORDS = [
    "ã‚¯ãƒ¼ãƒãƒ³", "ã‚³ãƒ¼ãƒ‰", "åŠé¡", "ã‚»ãƒ¼ãƒ«", "ç„¡æ–™", "å‰²å¼•", 
    "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³", "æ¿€å®‰", "ç‰¹ä¾¡", "é…å¸ƒ", "å††OFF", "ãƒã‚¤ãƒ³ãƒˆ",
    "ç™ºå£²", "é–‹å§‹", "ç™»å ´", "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ"
]

def load_sent_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def save_sent_data(data):
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data[-300:], f, ensure_ascii=False, indent=2)

def extract_image(entry):
    """RSSã‹ã‚‰ç”»åƒURLã‚’æŠœãå‡ºã™"""
    # 1. media_content (PR Timesãªã©)
    if 'media_content' in entry and len(entry.media_content) > 0:
        return entry.media_content[0]['url']
    # 2. links (ä¸€éƒ¨ã®ãƒ–ãƒ­ã‚°)
    if 'links' in entry:
        for link in entry.links:
            if link.get('type', '').startswith('image'):
                return link['href']
    # 3. descriptionå†…ã®imgã‚¿ã‚°
    if 'summary' in entry:
        soup = BeautifulSoup(entry.summary, 'html.parser')
        img = soup.find('img')
        if img and img.get('src'):
            return img['src']
    return None

def extract_coupon_code(text):
    """
    ã‚³ãƒ¼ãƒ‰æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå³ã—ã‚ã«åˆ¤å®šã—ã€ã‚´ãƒŸã‚’æ’é™¤ï¼‰
    """
    # æ˜ç¢ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŒ‡å®šãŒã‚ã‚‹å ´åˆ
    keyword_pattern = r'(?:ã‚¯ãƒ¼ãƒãƒ³|ã‚³ãƒ¼ãƒ‰|Key|ID)[:ï¼š]\s*([a-zA-Z0-9\-_]{4,20})'
    match = re.search(keyword_pattern, text, re.IGNORECASE)
    if match:
        code = match.group(1)
        if not re.search(r'(202[0-9]|http|jpg|png)', code):
            return code

    # Amazonãªã©ã§ã‚ˆãã‚ã‚‹ã€Œå¤§æ–‡å­—è‹±æ•°å­—ã®ç¾…åˆ—ã€
    # æ¡ä»¶: 6æ–‡å­—ä»¥ä¸Šã€å…¨éƒ¨å¤§æ–‡å­—ã€è‹±å­—ã¨æ•°å­—ãŒæ··ã–ã£ã¦ã„ã‚‹ã“ã¨
    general_pattern = r'\b(?=[A-Z0-9]*[A-Z])(?=[A-Z0-9]*[0-9])[A-Z0-9]{6,15}\b'
    matches = re.findall(general_pattern, text)
    
    ignore_list = ["IPHONE", "ANDROID", "WINDOWS", "TOKYO", "JAPAN", "UPDATE"]
    for m in matches:
        if m not in ignore_list and not m.startswith("202"):
            return m
            
    return None # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneã‚’è¿”ã™

def get_color(source_name):
    """ã‚µã‚¤ãƒˆã”ã¨ã«è‰²ã‚’å¤‰ãˆã‚‹"""
    name = source_name.lower()
    if "pr times" in name: return 0x1E90FF # Blue
    if "gizmodo" in name: return 0xFFD700 # Gold
    if "touch" in name: return 0xFF69B4 # Pink
    if "rocket" in name: return 0xDC143C # Red
    return 0x00FA9A # Default Green

def send_discord_embed(title, code, link, source_name, image_url, date_str):
    """ãƒªãƒƒãƒãªEmbedå½¢å¼ã§é€ä¿¡"""
    
    # èª¬æ˜æ–‡ã®ä½œæˆ
    description = ""
    if code:
        description = f"ğŸ”¥ **æ¿€ã‚¢ãƒ„ï¼ã‚¯ãƒ¼ãƒãƒ³ã‚³ãƒ¼ãƒ‰ç™ºè¦‹**\n```{code}```\nã“ã“ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ã£ã¦ã­ï¼"
        color = 0xFF0000 # èµ¤ï¼ˆå¼·èª¿ï¼‰
    else:
        description = "ğŸ‘‡ **ã‚»ãƒ¼ãƒ«ãƒ»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³è©³ç´°**\nã‚³ãƒ¼ãƒ‰ã¯ä¸è¦ã€ã‚‚ã—ãã¯ãƒªãƒ³ã‚¯å…ˆã§ãƒã‚§ãƒƒã‚¯ï¼"
        color = get_color(source_name)

    embed = {
        "title": title,
        "url": link,
        "description": description,
        "color": color,
        "footer": {
            "text": f"{source_name} â€¢ {date_str}"
        }
    }

    if image_url:
        embed["image"] = {"url": image_url}

    payload = {
        "username": "æ¿€å®‰ãƒãƒ³ã‚¿ãƒ¼Bot",
        "embeds": [embed]
    }

    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload)
        time.sleep(2)
    except Exception as e:
        print(f"Discord send error: {e}")

def main():
    if not DISCORD_WEBHOOK_URL:
        print("Webhook URL error")
        return

    sent_urls = load_sent_data()
    new_sent_urls = sent_urls.copy()
    
    print("Fetching feeds...")

    for rss_url in RSS_URLS:
        try:
            resp = requests.get(rss_url, headers=HEADERS, timeout=10)
            feed = feedparser.parse(resp.content)
            
            source_name = feed.feed.title if 'title' in feed.feed else "News"
            print(f"Checking: {source_name}")

            for entry in feed.entries[:5]: # æœ€æ–°5ä»¶
                link = entry.link
                title = entry.title
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if link in sent_urls: continue

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                if not any(k in title for k in TARGET_KEYWORDS): continue

                print(f"  -> HIT: {title}")

                # æœ¬æ–‡è§£æ
                description = ""
                if 'content' in entry: description = entry.content[0].value
                elif 'summary' in entry: description = entry.summary
                else: description = title
                
                soup = BeautifulSoup(description, "html.parser")
                text_content = soup.get_text()
                
                # æƒ…å ±æŠ½å‡º
                code = extract_coupon_code(text_content)
                image_url = extract_image(entry)
                
                # æ—¥ä»˜
                date_str = time.strftime('%Y-%m-%d %H:%M')

                # Discordé€ä¿¡
                send_discord_embed(title, code, link, source_name, image_url, date_str)
                
                new_sent_urls.append(link)

        except Exception as e:
            print(f"Error: {e}")
            continue

    save_sent_data(new_sent_urls)
    print("Done.")

if __name__ == "__main__":
    main()
