import feedparser
import requests
import json
import os
import re
from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta
from time import mktime

# --- è¨­å®šã‚¨ãƒªã‚¢ ---
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
DATA_FILE = "data.json"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆï¼ˆã“ã“ã«RSSHubã§ä½œã£ãŸTwitterã®RSSã‚’å…¥ã‚Œã‚‹ã¨æœ€å¼·ã«ãªã‚Šã¾ã™ï¼‰
RSS_URLS = [
    "https://prtimes.jp/gourmet.rdf",
    "https://prtimes.jp/technology.rdf",
    "https://touchlab.jp/feed/",
    "https://www.gizmodo.jp/index.xml",
    "https://corriente.top/feed/",
    "https://automaton-media.com/feed/",
    "https://rocketnews24.com/feed/",
]

# ã“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ãªã„ã¨ã€Œæ¿€ã‚¢ãƒ„ã€ã¨ã¿ãªã•ãªã„
HOT_KEYWORDS = [
    "ã‚¯ãƒ¼ãƒãƒ³", "ã‚³ãƒ¼ãƒ‰", "åŠé¡", "ç„¡æ–™", "å‰²å¼•", "æ¿€å®‰", "ç‰¹ä¾¡", 
    "å††OFF", "ãƒã‚¤ãƒ³ãƒˆ", "å…¨å“¡", "é…å¸ƒ", "ã‚¨ãƒ©ãƒ¼", "ãƒŸã‚¹"
]

def load_sent_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: return []
    return []

def save_sent_data(data):
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data[-300:], f, ensure_ascii=False, indent=2)

def is_within_24h(entry):
    """è¨˜äº‹ãŒ24æ™‚é–“ä»¥å†…ã‹åˆ¤å®š"""
    if not hasattr(entry, 'published_parsed'):
        return True # æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯å¿µã®ãŸã‚é€šã™
    
    published_time = datetime.fromtimestamp(mktime(entry.published_parsed))
    now = datetime.now()
    
    # 24æ™‚é–“ä»¥å†…ãªã‚‰True
    return (now - published_time) < timedelta(hours=24)

def deep_dive_for_code(url):
    """ãƒªãƒ³ã‚¯å…ˆã«å®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’æ¢ã™ï¼ˆDeep Scanï¼‰"""
    try:
        # è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å°‘ã—å¾…ã¤
        time.sleep(1)
        resp = requests.get(url, headers=HEADERS, timeout=5)
        if resp.status_code != 200: return None
        
        soup = BeautifulSoup(resp.content, 'html.parser')
        text = soup.get_text()
        
        # å³å¯†ãªã‚³ãƒ¼ãƒ‰æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
        return extract_strict_code(text)
    except:
        return None

def extract_strict_code(text):
    """
    ç²¾åº¦å„ªå…ˆã®ã‚³ãƒ¼ãƒ‰æŠ½å‡º
    """
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã€Œã‚³ãƒ¼ãƒ‰ï¼šXXXXã€ã¨ã„ã†æ˜ç¢ºãªæŒ‡å®š
    pattern1 = r'(?:ã‚¯ãƒ¼ãƒãƒ³|ã‚³ãƒ¼ãƒ‰|Key|ID)[:ï¼š]\s*([a-zA-Z0-9\-_]{4,20})'
    match = re.search(pattern1, text)
    if match:
        code = match.group(1)
        if not re.search(r'(202[0-9]|http|jpg|png)', code):
            return code

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: Amazonãƒ—ãƒ­ãƒ¢ã‚³ãƒ¼ãƒ‰é¢¨ï¼ˆå¤§æ–‡å­—è‹±æ•°å­—6æ¡ä»¥ä¸Šæ··åˆï¼‰
    # â€»èª¤æ¤œçŸ¥ã‚’æ¸›ã‚‰ã™ãŸã‚ã€å‰å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚‹ã‚‚ã®é™å®š
    pattern2 = r'\s([A-Z0-9]{6,12})\s'
    matches = re.findall(pattern2, text)
    
    ignore_list = ["IPHONE", "ANDROID", "WINDOWS", "UPDATE", "MOBILE", "TOKYO", "ONLINE"]
    
    for m in matches:
        # æ•°å­—ã¨æ–‡å­—ãŒä¸¡æ–¹æ··ã–ã£ã¦ã„ã‚‹ã“ã¨ï¼ˆç´”ç²‹ãªå˜èªã‚„æ•°å­—ã ã‘ã‚’é™¤å¤–ï¼‰
        if re.search(r'[A-Z]', m) and re.search(r'[0-9]', m):
            if m not in ignore_list:
                return m
    return None

def send_discord_embed(title, code, link, source_name, date_str):
    """ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã ã‘æ¿€ã‚¢ãƒ„é€šçŸ¥"""
    
    # ã‚³ãƒ¼ãƒ‰ãŒãªã„è¨˜äº‹ã¯ä»Šå›ã¯ç„¡è¦–ã™ã‚‹ï¼ï¼ˆã¾ãŸã¯ç°¡æ˜“é€šçŸ¥ã«ã™ã‚‹ï¼‰
    if not code:
        # ã‚³ãƒ¼ãƒ‰ãŒãªãã¦ã‚‚ã€ŒåŠé¡ã€ã¨ã‹æ›¸ã„ã¦ã‚ã£ãŸã‚‰é€šçŸ¥ã™ã‚‹æ•‘æ¸ˆæªç½®
        if "åŠé¡" in title or "ç„¡æ–™" in title:
            description = "ğŸ‘€ ã‚³ãƒ¼ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€æ¿€ã‚¢ãƒ„ã®æ°—é…ãŒã—ã¾ã™ï¼"
            color = 0xFFA500 # ã‚ªãƒ¬ãƒ³ã‚¸
        else:
            return # ã‚³ãƒ¼ãƒ‰ã‚‚ãªãã¦æ¿€ã‚¢ãƒ„ãƒ¯ãƒ¼ãƒ‰ã‚‚ãªã‘ã‚Œã°é€šçŸ¥ã—ãªã„ï¼ˆç„¡è¦–ï¼‰

    else:
        description = f"ğŸ”¥ **æ¿€ã‚¢ãƒ„ã‚³ãƒ¼ãƒ‰æŠ½å‡ºæˆåŠŸ**\n```{code}```\næ€¥ã„ã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼"
        color = 0xFF0000 # èµ¤ï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰

    embed = {
        "title": f"âš¡ {title}",
        "url": link,
        "description": description,
        "color": color,
        "footer": {
            "text": f"{source_name} â€¢ {date_str} (24hä»¥å†…)"
        }
    }

    payload = {
        "username": "DeepCoupon Hunter",
        "embeds": [embed]
    }

    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload)
        time.sleep(2)
    except Exception as e:
        print(f"Discord send error: {e}")

def main():
    if not DISCORD_WEBHOOK_URL:
        print("Webhook URL error")
        return

    sent_urls = load_sent_data()
    new_sent_urls = sent_urls.copy()
    
    print("Fetching feeds with Deep Scan...")

    for rss_url in RSS_URLS:
        try:
            resp = requests.get(rss_url, headers=HEADERS, timeout=10)
            feed = feedparser.parse(resp.content)
            source_name = feed.feed.title if 'title' in feed.feed else "News"
            
            # å„ã‚µã‚¤ãƒˆæœ€æ–°5ä»¶ã ã‘ãƒã‚§ãƒƒã‚¯ï¼ˆæ·±å±¤ã‚¹ã‚­ãƒ£ãƒ³ã®è² è·è»½æ¸›ã®ãŸã‚ï¼‰
            for entry in feed.entries[:5]:
                link = entry.link
                title = entry.title
                
                # 1. é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if link in sent_urls: continue

                # 2. æ™‚é–“ãƒã‚§ãƒƒã‚¯ï¼ˆ24æ™‚é–“ä»¥å†…ã‹ï¼Ÿï¼‰
                if not is_within_24h(entry):
                    # å¤ã„è¨˜äº‹ã¯ç„¡è¦–
                    continue

                # 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€æ¬¡å¯©æŸ»
                if not any(k in title for k in HOT_KEYWORDS):
                    continue

                print(f"  ğŸ” Deep Scanning: {title}...")

                # 4. æ·±å±¤ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆãƒªãƒ³ã‚¯å…ˆã«è¡Œã£ã¦ã‚³ãƒ¼ãƒ‰ã‚’æ¢ã™ï¼‰
                # ã¾ãšæ¦‚è¦ã‹ã‚‰æ¢ã™
                description = entry.summary if 'summary' in entry else title
                code = extract_strict_code(BeautifulSoup(description, "html.parser").get_text())
                
                # æ¦‚è¦ã«ãªã‘ã‚Œã°ã€ãƒªãƒ³ã‚¯å…ˆã«çªæ’ƒ
                if not code:
                    code = deep_dive_for_code(link)

                # ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã£ãŸã€ã‚‚ã—ãã¯ã‚¿ã‚¤ãƒˆãƒ«ãŒè¶…å¼·åŠ›ãªã‚‰é€šçŸ¥
                send_discord_embed(title, code, link, source_name, "Just Now")
                
                new_sent_urls.append(link)

        except Exception as e:
            print(f"Error checking {rss_url}: {e}")
            continue

    save_sent_data(new_sent_urls)
    print("Done.")

if __name__ == "__main__":
    main()
